{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cc750b-78e8-42dc-a64f-a4c679203e68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.199.0.tar.gz (1.0 MB)\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting boto3<2.0,>=1.33.3\n",
      "  Downloading boto3-1.33.6-py3-none-any.whl (139 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (1.21.5)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (3.19.4)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (1.4.2)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting PyYAML~=6.0\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "Requirement already satisfied: jsonschema in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (4.4.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.9/dist-packages (from sagemaker) (2.6.2)\n",
      "Collecting tblib==1.7.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<1.27 in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (1.26.9)\n",
      "Collecting uvicorn==0.22.0\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "Collecting fastapi==0.95.2\n",
      "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (2.27.1)\n",
      "Collecting docker\n",
      "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.9/site-packages (from sagemaker) (5.8.0)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.10.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.9/site-packages (from uvicorn==0.22.0->sagemaker) (8.0.4)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "Collecting botocore<1.34.0,>=1.33.6\n",
      "  Downloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.9/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.34.0,>=1.33.6->boto3<2.0,>=1.33.3->sagemaker) (2.8.2)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.6->boto3<2.0,>=1.33.3->sagemaker) (1.16.0)\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-4.1.0-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (3.3)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->sagemaker) (2021.10.8)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Collecting ppft>=1.7.6.7\n",
      "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
      "Collecting multiprocess>=0.70.15\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "Collecting pox>=0.3.3\n",
      "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
      "Collecting dill>=0.3.7\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.199.0-py2.py3-none-any.whl size=1374439 sha256=c830bce0acb54fa2ebff66a391a4295f615d439c2623ff43a722c734acdd8c92\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/de/dd/64b6bd4608def48b0794bd61014fffb1a7fa0c6f889eb036eb\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sniffio, exceptiongroup, typing-extensions, dill, botocore, anyio, zipp, websocket-client, starlette, s3transfer, pydantic, ppft, pox, multiprocess, h11, contextlib2, attrs, uvicorn, tqdm, tblib, smdebug-rulesconfig, schema, PyYAML, pathos, importlib-metadata, google-pasta, fastapi, docker, cloudpickle, boto3, sagemaker\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.32\n",
      "    Not uninstalling botocore at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52\n",
      "    Can't uninstall 'botocore'. No files were found to uninstall.\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Not uninstalling s3transfer at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52\n",
      "    Can't uninstall 's3transfer'. No files were found to uninstall.\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Not uninstalling attrs at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52\n",
      "    Can't uninstall 'attrs'. No files were found to uninstall.\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.21.32\n",
      "    Not uninstalling boto3 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52\n",
      "    Can't uninstall 'boto3'. No files were found to uninstall.\n",
      "Successfully installed PyYAML-6.0.1 anyio-4.1.0 attrs-23.1.0 boto3-1.33.6 botocore-1.33.6 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.7 docker-6.1.3 exceptiongroup-1.2.0 fastapi-0.95.2 google-pasta-0.2.0 h11-0.14.0 importlib-metadata-6.11.0 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 pydantic-1.10.13 s3transfer-0.8.2 sagemaker-2.199.0 schema-0.7.5 smdebug-rulesconfig-1.0.1 sniffio-1.3.0 starlette-0.27.0 tblib-1.7.0 tqdm-4.66.1 typing-extensions-4.8.0 uvicorn-0.22.0 websocket-client-1.7.0 zipp-3.17.0\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb43e2ff-3ff8-4529-a342-7de24b4fccd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: pymongo[srv]==3.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52/lib/python3.9/site-packages (3.11.0)\n",
      "Requirement already satisfied: dnspython<2.0.0,>=1.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7c439d67-d10c-46c6-9f27-a13f33890a52/lib/python3.9/site-packages (from pymongo[srv]==3.11) (1.16.0)\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.9/site-packages (2021.10.8)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pymongo[srv]\"==3.11\n",
    "%pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1381da64-bd1d-451b-b68f-9d232601dc1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchPredictor\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import base64\n",
    "from pymongo.mongo_client import MongoClient\n",
    "import certifi\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AKIA2NAYNXIWDNXIOWXDGNNAFMCOT4P',\n",
    "    aws_secret_access_key='SRd5VKB4NXUDWXIIWJUbegRyZlEC+XzJ03m8ogIBwkN+0mXmnY',\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "sm = sagemaker.Session(boto_session=session)\n",
    "\n",
    "sm_client = session.client(service_name=\"sagemaker\",)\n",
    "\n",
    "# Restore the endpoint name stored in the 2_DeployEndpoint.ipynb notebook\n",
    "predictor = PyTorchPredictor(endpoint_name=\"yolov8-pytorch-2023-12-03-23-50-32-390066\",\n",
    "                             deserializer=JSONDeserializer(),sagemaker_session = sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90024b6f-0b14-4f68-a712-55355c3013d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "confluentBootstrapserver = \"pkc-p11xm.us-east-1.aws.confluent.cloud:9092\"\n",
    "confluentTopic = \"video_frames\"\n",
    "apiKey=\"DHKC6KBU7QMBU6NIXKW6A\"\n",
    "apiSecret=\"GNanft1XNOWDNXIOWDozcvfMunJk5Je/ANQmtNmh0UF1XuYmIjJQYwUc55ElnY8qUbtZ/A1xAuB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228cbb04-531a-42a9-a321-33564be8bf0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapserver) \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{apiKey}\" password=\"{apiSecret}\";') \\\n",
    "    .option(\"subscribe\", confluentTopic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96025b9-5487-4b82-9771-c5eef533f1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"key\", col(\"key\").cast(\"string\")) \\\n",
    "       .withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# Define the schema that matches the expected structure of your JSON\n",
    "# For example, if your JSON has 'camera_id', 'frame_base64', and 'timestamp' fields at the top level\n",
    "schema = StructType([\n",
    "    StructField(\"camera_id\", StringType(), True),\n",
    "    StructField(\"frame_base64\", StringType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON data based on the adjusted schema\n",
    "parsed_df = df.select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "# Extracting fields from the parsed data\n",
    "parsed_df = parsed_df.select(\"data.*\")\n",
    "parsed_df = parsed_df.withColumn(\"timestamp\", parsed_df[\"timestamp\"].cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37386ad-0dd8-4b3d-bd1b-7517e8fc29e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "def perform_inference(rows):\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        print(\"##########################################\",row.asDict().keys())\n",
    "        camera_id = row.camera_id\n",
    "        frames = row.frames\n",
    "    # camera_id = row.camera_id\n",
    "    # frames = row.frames\n",
    "        timestamp =  row.window.start\n",
    "        print(f\"Performing inference for camera {camera_id} with frames: {len(frames)}\")\n",
    "        \n",
    "        # Initialize the SageMaker session and predictor\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id='AKIA2NAYDGNNAFMCOT4PNJS',\n",
    "            aws_secret_access_key='SRd5VKB4UbegRyZlECMKXSO+XzJ03m8ogIBwkN+0mXmnY',\n",
    "            region_name=\"us-east-1\"\n",
    "        )\n",
    "        sm = sagemaker.Session(boto_session=session)\n",
    "        predictor = PyTorchPredictor(endpoint_name=\"yolov8-pytorch-2023-12-03-23-50-32-390066\",\n",
    "                                    deserializer=JSONDeserializer(), sagemaker_session=sm)\n",
    "        infer_results = []\n",
    "        for frame_base64 in frames:\n",
    "            # Convert base64 to image_bytes if needed\n",
    "            image_bytes = base64.b64decode(frame_base64)\n",
    "            # Perform inference using 'image_bytes' with SageMaker predictor\n",
    "            result = predictor.predict(image_bytes)\n",
    "            infer_results.append(result)\n",
    "        results.append([camera_id, infer_results, timestamp])\n",
    "            # Process the result as needed\n",
    "    return results\n",
    "\n",
    "def write_to_mongodb(rows):\n",
    "    mongo_data = []\n",
    "    for row in rows:\n",
    "        camera_id, inference_results,timestamp = row\n",
    "        print(f\"Writing results for camera {camera_id} at timestamp {timestamp} to MongoDB\")\n",
    "        formatted_timestamp = timestamp.isoformat()\n",
    "\n",
    "        \n",
    "        for result in inference_results:\n",
    "            empty = 0\n",
    "            occupied = 0\n",
    "            for i in result['boxes']:\n",
    "                if int(i[0][5]) == 0:\n",
    "                    empty += 1\n",
    "                else:\n",
    "                    occupied += 1\n",
    "            mongo_data.append({\n",
    "                \"camera_id\": camera_id,\n",
    "                \"timestamp\": formatted_timestamp,\n",
    "                \"empty\": empty,\n",
    "                \"occupied\": occupied\n",
    "            })\n",
    "    \n",
    "    # Write to MongoDB (Replace this with your MongoDB write logic)\n",
    "    # For example, you can use PyMongo to write to MongoDB\n",
    "    if(len(mongo_data)>0):\n",
    "        mongo_client = MongoClient('mongodb+srv://gouravvemula:XXXXXXXX@cluster0.xxw6gfy.mongodb.net/?retryWrites=true&w=majority', tlsCAFile=certifi.where())\n",
    "        db = mongo_client['parking_db']\n",
    "        collection = db['parking']\n",
    "        # Insert data into MongoDB collection along with formatted timestamp\n",
    "        collection.insert_many(mongo_data)\n",
    "\n",
    "\n",
    "watermarked_df = parsed_df.withWatermark(\"timestamp\", \"1 minute\")\n",
    "\n",
    "# Apply grouping by window and camera_id\n",
    "grouped_df = watermarked_df \\\n",
    "    .groupBy(window(\"timestamp\", \"1 minute\"), \"camera_id\") \\\n",
    "    .agg(collect_list(\"frame_base64\").alias(\"frames\"))\n",
    "\n",
    "# Perform inference and write results to MongoDB within the streaming query\n",
    "mongo_query = grouped_df.writeStream.foreachBatch(\n",
    "    lambda batch_df, batch_id: write_to_mongodb(\n",
    "        perform_inference(batch_df.collect())\n",
    "    )\n",
    ").start()\n",
    "\n",
    "mongo_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-12-03 03:14:29",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
